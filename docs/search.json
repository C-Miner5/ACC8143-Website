{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-06T01:09:55-06:00"
    },
    {
      "path": "index.html",
      "title": "Colin Miner",
      "description": "Creating organic, GMO-free software since 1985",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Colin Miner - ACC8143\r\n          \r\n          \r\n          Home\r\n          Resume\r\n          \r\n          \r\n          Projects\r\n           \r\n          ▾\r\n          \r\n          \r\n          Rsquared\r\n          Machine Learning\r\n          Final Project\r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            Colin Miner\r\n          \r\n          \r\n            \r\n              I am a data wrangler, requirements gatherer, and project conductor.\r\n            \r\n            \r\n              I am a data wrangler, requirements gatherer, and project conductor.\r\n            \r\n          \r\n\r\n          \r\n            \r\n              \r\n                  \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      Email\r\n                    \r\n                  \r\n                \r\n                              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                                \r\n                  \r\n                    LinkedIn\r\n                  \r\n                \r\n                                \r\n                  \r\n                    GitHub\r\n                  \r\n                \r\n                                \r\n                  \r\n                    Email\r\n                  \r\n                \r\n                              \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-12-06T01:09:55-06:00"
    },
    {
      "path": "ML.html",
      "title": "Machine Learning",
      "description": "Random Forests\n",
      "author": [
        {
          "name": "Colin Miner",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\nImagine you are in a class of 50 students and the teacher is going to ask a question about the homework. If the teacher randomly picks one student there is a chance of picking someone who will give the wrong answer. But if the teacher has everyone answer the question, and then picks the answer with the most votes, there is a higher chance of getting the right answer.\r\nRandom Forests are very similar. They choose an answer that has the most votes out of a group of Decision Trees. A decision tree is a way of trying to predict something based off of data by answering True or False questions. For example if you had a basket of different foods and wanted a way to have a computer tell if any piece of food was an apple. You could first ask “Is it a fruit?” If the answer is no, then it isn’t an apple. But if the answer is yes, you could then ask “is it round?”, “is it red?”, “is it sweet?”, etc. A random forest takes lots of these decision trees, each using different sets of questions about the food, and takes the answer with the most votes. This way if any one decision tree is really wrong in how it predicts, it won’t tips the scales too much.\r\nIn the accounting world this can be used to try and predict customers who can be immediately approved for a loan, or which loans might need someone to take a closer look at the deal before approving it.\r\nThis example uses data about flowers to try and predict if any given flower is male or female.\r\n\r\n\r\n    set.seed(1)\r\n    #lets split the data 60/40\r\n     \r\n\r\n    trainIndex <- createDataPartition(tips$sex, p = .6, list = FALSE, times = 1)\r\n\r\n    #grab the data\r\n    tipsTrain <- tips[ trainIndex,]\r\n    tipsTest  <- tips[-trainIndex,]\r\n\r\n    set.seed(1)\r\n\r\n    tipsRF<- train(\r\n      form = factor(sex) ~ total_bill+tip+size,\r\n      data = tipsTrain,\r\n      #here we add classProbs because we want probs\r\n      trControl = trainControl(method = \"cv\", number = 10,\r\n                               classProbs =  TRUE),\r\n      method = \"rf\",\r\n      tuneLength = 2)\r\n\r\n    tipsRF_Pred<-predict(tipsRF,tipsTest,type=\"prob\")\r\n    tipsrftestpred<-cbind(tipsRF_Pred,tipsTest)\r\n    tipsRF\r\n\r\n\r\nRandom Forest \r\n\r\n148 samples\r\n  3 predictor\r\n  2 classes: 'Female', 'Male' \r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 133, 134, 133, 132, 134, 134, ... \r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy   Kappa      \r\n  2     0.5672619  0.009170983\r\n  3     0.5739286  0.042797266\r\n\r\nAccuracy was used to select the optimal model using the\r\n largest value.\r\nThe final value used for the model was mtry = 3.\r\n\r\n    tipsrftestpred<-tipsrftestpred%>%\r\n      mutate(prediction=ifelse(Male>=.5,\"Male\",\"Female\"))\r\n\r\n    confusionMatrix(factor(tipsrftestpred$prediction),factor(tipsrftestpred$sex))\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction Female Male\r\n    Female     10   18\r\n    Male       24   44\r\n                                          \r\n               Accuracy : 0.5625          \r\n                 95% CI : (0.4575, 0.6636)\r\n    No Information Rate : 0.6458          \r\n    P-Value [Acc > NIR] : 0.9636          \r\n                                          \r\n                  Kappa : 0.004           \r\n                                          \r\n Mcnemar's Test P-Value : 0.4404          \r\n                                          \r\n            Sensitivity : 0.2941          \r\n            Specificity : 0.7097          \r\n         Pos Pred Value : 0.3571          \r\n         Neg Pred Value : 0.6471          \r\n             Prevalence : 0.3542          \r\n         Detection Rate : 0.1042          \r\n   Detection Prevalence : 0.2917          \r\n      Balanced Accuracy : 0.5019          \r\n                                          \r\n       'Positive' Class : Female          \r\n                                          \r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-07T12:23:45-06:00"
    },
    {
      "path": "Resume.html",
      "title": "",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\nColin Miner\r\nSoftware Developer\r\n\r\ncjm131@msstate.edu\r\nSkills Summary\r\nEducational History\r\nData warehousing and analytics\r\nUniversity of Southern Mississippi\r\nSoftware Development\r\nBachelor of Science, Computer Science\r\nProcess Improvement\r\nHonors College\r\nProject Management\r\nMay 2007\r\nVB/C#/SQL\r\n\r\nCrystal Reports\r\n\r\nWork Experience\r\nBlossman Gas, Inc\r\nJune 2006 - Present\r\nResponsible for all aspects of application development including requirements gathering, design, development,testing, implementation, and maintenance\r\nTrain team members in company best practices and programming techniques\r\nCoordinate programming projects with other departments\r\nRegularly deliver financial and productivity reports to company officers\r\nTroubleshoot data and software problems\r\nAutomated and maintain the generation, printing, and mailing of company billing documents\r\nCreated an SMS texting process to alert customers of upcoming/completed deliveries and handle incoming customer text requests, reducing workload on call center personnel\r\nDeveloped multiple interfaces to parse payment and delivery data and post to back office software\r\nCreated internal tool suite including customer financing applications, propane hedging management, parts ordering, and interface administration Responsible for analyzing and integrating data from company acquisitions\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-06T01:09:56-06:00"
    },
    {
      "path": "RSquared.html",
      "title": "R Squared",
      "description": "R Squared as a measure of model fit\n",
      "author": [
        {
          "name": "Colin Miner",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nWhen a measure indicates that a model is performing poorly, it will prompt the user to investigate the model for problems. This may waste time, but is less of a problem than if the measure gives a false positive and makes it seem like the model is performing well. This can cause a user to continue on with the bad model and create more problems down the road.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n\r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nEven adjusted and predictive R^2 are not much better:\r\n\r\n\r\n#Adjusted R^2\r\nsummary(lm(y ~ x))$adj.r.squared\r\n\r\n\r\n[1] 0.8453587\r\n\r\n#PRESS - predicted residual sums of squares\r\n\r\nPRESS <- function(linear.model) {\r\n  #' calculate the predictive residuals\r\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\r\n  #' calculate the PRESS\r\n  PRESS <- sum(pr^2)\r\n  \r\n  return(PRESS)\r\n}\r\n\r\npred_r_squared <- function(linear.model) {\r\n  #' Use anova() to get the sum of squares for the linear model\r\n  lm.anova <- anova(linear.model)\r\n  #' Calculate the total sum of squares\r\n  tss <- sum(lm.anova$'Sum Sq')\r\n  # Calculate the predictive R^2\r\n  pred.r.squared <- 1-PRESS(linear.model)/(tss)\r\n  \r\n  return(pred.r.squared)\r\n}\r\n\r\nmodel <- lm(y ~ x)  \r\npred_r_squared(model)\r\n\r\n\r\n[1] 0.7700853\r\n\r\nThere is no perfect solution, but relying on a single measure for model fit always leaves a chance of something going wrong. Using multiple measures helps, as does simply graphing the data and visually judging the model.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-06T01:55:21-06:00"
    }
  ],
  "collections": []
}
